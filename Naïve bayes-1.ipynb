{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec74020-5f2b-4d99-a312-bcb68a381012",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Bayes' theorem?\n",
    "Bayes' theorem is a fundamental principle in probability theory that describes how to update the probability of a hypothesis based on new evidence. It provides a way to calculate the probability of a certain event occurring given the probability of another related event. In essence, it connects prior knowledge with new data to improve predictions or decisions.\n",
    "Q2. What is the formula for Bayes' theorem?\n",
    "The formula for Bayes' theorem is:\n",
    "P(A∣B)=P(B∣A)⋅P(A)P(B)P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}P(A∣B)=P(B)P(B∣A)⋅P(A)\n",
    "Where:\n",
    "•\tP(A∣B)P(A|B)P(A∣B) is the posterior probability of event A given that B has occurred.\n",
    "•\tP(B∣A)P(B|A)P(B∣A) is the likelihood of event B given that A is true.\n",
    "•\tP(A)P(A)P(A) is the prior probability of event A.\n",
    "•\tP(B)P(B)P(B) is the marginal probability of event B.\n",
    "Q3. How is Bayes' theorem used in practice?\n",
    "Bayes' theorem is widely used in various applications, including:\n",
    "•\tMedical Diagnosis: To update the probability of a disease based on test results.\n",
    "•\tSpam Filtering: To classify emails as spam or not based on features such as words or phrases.\n",
    "•\tRisk Assessment: To evaluate the probability of risk based on prior knowledge and new evidence.\n",
    "•\tRecommendation Systems: To provide personalized recommendations based on user behavior and preferences.\n",
    "Q4. What is the relationship between Bayes' theorem and conditional probability?\n",
    "Bayes theorem is based on conditional probability, which measures the probability of an event occurring given that another event has already occurred. Bayes' theorem provides a method for calculating the conditional probability P(A∣B)P(A|B)P(A∣B) by incorporating the likelihood of the new evidence P(B∣A)P(B|A)P(B∣A), the prior probability P(A)P(A)P(A), and the overall probability of the evidence P(B)P(B)P(B).\n",
    "Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
    "The choice of Naive Bayes classifier depends on the nature of the features in the dataset:\n",
    "•\tGaussian Naive Bayes: Used when features are continuous and assumed to follow a Gaussian (normal) distribution.\n",
    "•\tMultinomial Naive Bayes: Suitable for discrete features and is often used in text classification where the features represent word counts or frequencies.\n",
    "•\tBernoulli Naive Bayes: Ideal for binary/boolean features, where the features indicate the presence or absence of certain attributes.\n",
    "Q6. Assignment: Classification with Naive Bayes\n",
    "Given the following frequency table:\n",
    "Class\tX1=1\tX1=2\tX1=3\tX2=1\tX2=2\tX2=3\tX2=4\n",
    "A\t3\t3\t4\t4\t3\t3\t3\n",
    "B\t2\t2\t1\t2\t2\t2\t3\n",
    "Assuming equal prior probabilities for each class, we will use the Naive Bayes classifier to predict the class for a new instance with X1=3X1 = 3X1=3 and X2=4X2 = 4X2=4.\n",
    "Steps to Classify:\n",
    "1.\tCalculate the prior probability for each class:\n",
    "Since the prior probabilities are equal, P(A)=P(B)=0.5P(A) = P(B) = 0.5P(A)=P(B)=0.5.\n",
    "2.\tCalculate the likelihood of the features given each class:\n",
    "For class A:\n",
    "o\tP(X1=3∣A)=Frequency of X1=3 in ATotal frequency of X1 in A=43+3+4=410=0.4P(X1 = 3 | A) = \\frac{\\text{Frequency of } X1 = 3 \\text{ in } A}{\\text{Total frequency of } X1 \\text{ in } A} = \\frac{4}{3+3+4} = \\frac{4}{10} = 0.4P(X1=3∣A)=Total frequency of X1 in AFrequency of X1=3 in A=3+3+44=104=0.4\n",
    "o\tP(X2=4∣A)=Frequency of X2=4 in ATotal frequency of X2 in A=34+3+3+3=313≈0.231P(X2 = 4 | A) = \\frac{\\text{Frequency of } X2 = 4 \\text{ in } A}{\\text{Total frequency of } X2 \\text{ in } A} = \\frac{3}{4+3+3+3} = \\frac{3}{13} \\approx 0.231P(X2=4∣A)=Total frequency of X2 in AFrequency of X2=4 in A=4+3+3+33=133≈0.231\n",
    "For class B:\n",
    "o\tP(X1=3∣B)=Frequency of X1=3 in BTotal frequency of X1 in B=12+2+1=15=0.2P(X1 = 3 | B) = \\frac{\\text{Frequency of } X1 = 3 \\text{ in } B}{\\text{Total frequency of } X1 \\text{ in } B} = \\frac{1}{2+2+1} = \\frac{1}{5} = 0.2P(X1=3∣B)=Total frequency of X1 in BFrequency of X1=3 in B=2+2+11=51=0.2\n",
    "o\tP(X2=4∣B)=Frequency of X2=4 in BTotal frequency of X2 in B=32+2+2+3=39=0.333P(X2 = 4 | B) = \\frac{\\text{Frequency of } X2 = 4 \\text{ in } B}{\\text{Total frequency of } X2 \\text{ in } B} = \\frac{3}{2+2+2+3} = \\frac{3}{9} = 0.333P(X2=4∣B)=Total frequency of X2 in BFrequency of X2=4 in B=2+2+2+33=93=0.333\n",
    "3.\tCalculate the posterior probability for each class:\n",
    "For class A:\n",
    "P(A∣X1=3,X2=4)∝P(X1=3∣A)⋅P(X2=4∣A)⋅P(A)=0.4⋅0.231⋅0.5=0.023P(A | X1 = 3, X2 = 4) \\propto P(X1 = 3 | A) \\cdot P(X2 = 4 | A) \\cdot P(A) = 0.4 \\cdot 0.231 \\cdot 0.5 = 0.023P(A∣X1=3,X2=4)∝P(X1=3∣A)⋅P(X2=4∣A)⋅P(A)=0.4⋅0.231⋅0.5=0.023\n",
    "For class B:\n",
    "P(B∣X1=3,X2=4)∝P(X1=3∣B)⋅P(X2=4∣B)⋅P(B)=0.2⋅0.333⋅0.5=0.033P(B | X1 = 3, X2 = 4) \\propto P(X1 = 3 | B) \\cdot P(X2 = 4 | B) \\cdot P(B) = 0.2 \\cdot 0.333 \\cdot 0.5 = 0.033P(B∣X1=3,X2=4)∝P(X1=3∣B)⋅P(X2=4∣B)⋅P(B)=0.2⋅0.333⋅0.5=0.033\n",
    "Since P(B∣X1=3,X2=4)>P(A∣X1=3,X2=4)P(B | X1 = 3, X2 = 4) > P(A | X1 = 3, X2 = 4)P(B∣X1=3,X2=4)>P(A∣X1=3,X2=4), the Naive Bayes classifier would predict that the new instance belongs to class B.\n",
    "4o mini\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
